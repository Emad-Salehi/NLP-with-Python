{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:20px\">#Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nooshin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Nooshin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id           0\n",
      "title      558\n",
      "author    1957\n",
      "text        39\n",
      "label        0\n",
      "dtype: int64\n",
      "(20761, 5)\n",
      "(20386, 5)\n",
      "confusion matrix for the model : \n",
      "[[1929  145]\n",
      " [ 283 1721]]\n",
      "\n",
      "classification report for the model : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.93      0.90      2074\n",
      "           1       0.92      0.86      0.89      2004\n",
      "\n",
      "    accuracy                           0.90      4078\n",
      "   macro avg       0.90      0.89      0.89      4078\n",
      "weighted avg       0.90      0.90      0.89      4078\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk import corpus\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load the dataset\n",
    "news_df = pd.read_csv ('news.csv')\n",
    "\n",
    "# Preprocess\n",
    "print(news_df.isnull().sum()) # so we understand that we have 39 null text\n",
    "news_df = news_df[news_df[\"text\"].notnull()] # remove rows with null value for text column\n",
    "news_df.drop_duplicates(subset=['text', 'label'],keep=\"first\",inplace = True) # we understand that we have 419  duplicated text so we remove them\n",
    "\n",
    "\n",
    "## part (A)\n",
    "# a) lowercase text\n",
    "news_df[\"text\"] = news_df[\"text\"].str.lower()\n",
    "\n",
    "# b) remove digits\n",
    "news_df[\"text\"] =  news_df[\"text\"].str.replace('\\d+', '')\n",
    "\n",
    "# c) remove Punctuations and special characters\n",
    "news_df[\"text\"] =  news_df[\"text\"].str.translate(str.maketrans('','', string.punctuation))\n",
    "news_df[\"text\"] =  news_df[\"text\"].str.translate(str.maketrans('', '', '''’—“”»«›!©…•–'''))\n",
    "\n",
    "# d) remove single characters\n",
    "news_df[\"text\"] = news_df[\"text\"].apply(lambda x: ' '.join(word for word in x.split() if len(word)>1 ))\n",
    "\n",
    "# remove newline and tab\n",
    "news_df[\"text\"] =  news_df[\"text\"].str.replace('\\n|\\t', ' ')\n",
    "\n",
    "# e,f) Lemmatization, Stemming and removing stop words\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "cachedStopWords  = stopwords.words('english')\n",
    "news_df[\"text\"] = news_df[\"text\"].apply(lambda x: ' '.join([stemmer.stem(lemmatizer.lemmatize(word)) for word in x.split() if word not in cachedStopWords]))\n",
    "\n",
    "## part (B)\n",
    "# Creating the Bag of Words\n",
    "corpus = news_df[\"text\"].tolist()\n",
    "cv = CountVectorizer(max_features=6000, ngram_range=(1,3))\n",
    "x = cv.fit_transform(corpus).toarray()\n",
    "y = news_df[\"label\"]\n",
    "\n",
    "## part (C)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2500, shuffle=True)\n",
    "\n",
    "## part (D)\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(x_train, y_train)\n",
    "y_pred = classifier.predict(x_test)\n",
    "\n",
    "## part (E)\n",
    "# printing confusion matrix for the model\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nconfusion matrix for the model : \\n\" + str(cm))\n",
    "# printing classification report for the model\n",
    "cr = classification_report(y_test, y_pred)\n",
    "print(\"\\nclassification report for the model : \\n\" + str(cr)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:20px\">#Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nooshin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>From: dfo@vttoulu.tko.vtt.fi (Foxvog Douglas)\\...</td>\n",
       "      <td>16</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>From: bmdelane@quads.uchicago.edu (brian manni...</td>\n",
       "      <td>13</td>\n",
       "      <td>sci.med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>From: bgrubb@dante.nmsu.edu (GRUBB)\\nSubject: ...</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>From: holmes7000@iscsvax.uni.edu\\nSubject: WIn...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>From: kerr@ux1.cso.uiuc.edu (Stan Kerr)\\nSubje...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  target  \\\n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4   \n",
       "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1   \n",
       "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14   \n",
       "5  From: dfo@vttoulu.tko.vtt.fi (Foxvog Douglas)\\...      16   \n",
       "6  From: bmdelane@quads.uchicago.edu (brian manni...      13   \n",
       "7  From: bgrubb@dante.nmsu.edu (GRUBB)\\nSubject: ...       3   \n",
       "8  From: holmes7000@iscsvax.uni.edu\\nSubject: WIn...       2   \n",
       "9  From: kerr@ux1.cso.uiuc.edu (Stan Kerr)\\nSubje...       4   \n",
       "\n",
       "               target_names  \n",
       "0                 rec.autos  \n",
       "1     comp.sys.mac.hardware  \n",
       "2     comp.sys.mac.hardware  \n",
       "3             comp.graphics  \n",
       "4                 sci.space  \n",
       "5        talk.politics.guns  \n",
       "6                   sci.med  \n",
       "7  comp.sys.ibm.pc.hardware  \n",
       "8   comp.os.ms-windows.misc  \n",
       "9     comp.sys.mac.hardware  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.phrases import Phrases, FrozenPhrases\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "## part (A)\n",
    "# Load dataset\n",
    "df = pd.read_json(\"https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json\")\n",
    "df.head(10)\n",
    "#print(df.shape) \n",
    "#print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.010*\"university\" + 0.007*\"game\" + 0.006*\"one\" + 0.006*\"like\" + 0.005*\"article\" + 0.005*\"writes\" + 0.004*\"win\" + 0.004*\"card\" + 0.004*\"time\" + 0.003*\"dont\"'), (1, '0.007*\"pts\" + 0.006*\"team\" + 0.005*\"players\" + 0.005*\"player\" + 0.004*\"one\" + 0.004*\"article\" + 0.004*\"time\" + 0.004*\"writes\" + 0.004*\"like\" + 0.004*\"aids\"'), (2, '0.008*\"information\" + 0.006*\"space\" + 0.006*\"encryption\" + 0.005*\"technology\" + 0.005*\"data\" + 0.004*\"one\" + 0.004*\"university\" + 0.004*\"new\" + 0.004*\"mail\" + 0.004*\"writes\"'), (3, '0.007*\"sale\" + 0.007*\"new\" + 0.006*\"one\" + 0.006*\"article\" + 0.006*\"like\" + 0.006*\"writes\" + 0.005*\"window\" + 0.004*\"price\" + 0.003*\"university\" + 0.003*\"way\"'), (4, '0.011*\"file\" + 0.009*\"entry\" + 0.006*\"program\" + 0.005*\"section\" + 0.004*\"entries\" + 0.004*\"information\" + 0.004*\"send\" + 0.004*\"files\" + 0.004*\"info\" + 0.004*\"faq\"'), (5, '0.012*\"people\" + 0.010*\"writes\" + 0.009*\"article\" + 0.006*\"one\" + 0.006*\"dont\" + 0.005*\"like\" + 0.004*\"government\" + 0.004*\"gun\" + 0.004*\"well\" + 0.003*\"state\"'), (6, '0.009*\"software\" + 0.007*\"program\" + 0.007*\"image\" + 0.007*\"available\" + 0.007*\"server\" + 0.006*\"graphics\" + 0.006*\"file\" + 0.006*\"code\" + 0.006*\"color\" + 0.005*\"version\"'), (7, '0.010*\"bike\" + 0.007*\"writes\" + 0.006*\"dod\" + 0.006*\"article\" + 0.005*\"dont\" + 0.005*\"dog\" + 0.004*\"oil\" + 0.004*\"road\" + 0.003*\"one\" + 0.003*\"well\"'), (8, '0.016*\"key\" + 0.007*\"one\" + 0.006*\"encryption\" + 0.006*\"writes\" + 0.006*\"ripem\" + 0.006*\"keys\" + 0.005*\"article\" + 0.005*\"system\" + 0.005*\"security\" + 0.004*\"public\"'), (9, '0.007*\"article\" + 0.007*\"like\" + 0.006*\"one\" + 0.006*\"writes\" + 0.005*\"moon\" + 0.005*\"high\" + 0.004*\"space\" + 0.004*\"earth\" + 0.004*\"mars\" + 0.004*\"audio\"'), (10, '0.068*\"max_max\" + 0.021*\"max\" + 0.010*\"rlk\" + 0.006*\"air\" + 0.005*\"zei\" + 0.005*\"bhj\" + 0.005*\"qax_max\" + 0.004*\"fij\" + 0.004*\"wwg\" + 0.003*\"giz\"'), (11, '0.011*\"x_s\" + 0.011*\"sc_\" + 0.010*\"scx\" + 0.007*\"captain\" + 0.007*\"xterm\" + 0.007*\"chz\" + 0.006*\"deskjet\" + 0.006*\"color\" + 0.006*\"cx_s\" + 0.005*\"mcx\"'), (12, '0.008*\"drive\" + 0.008*\"windows\" + 0.007*\"system\" + 0.007*\"problem\" + 0.007*\"one\" + 0.006*\"like\" + 0.005*\"writes\" + 0.005*\"article\" + 0.005*\"dont\" + 0.005*\"anyone\"'), (13, '0.011*\"jpeg\" + 0.009*\"insurance\" + 0.008*\"keyboard\" + 0.007*\"helmet\" + 0.006*\"finland\" + 0.006*\"canada\" + 0.006*\"medical\" + 0.005*\"health\" + 0.005*\"patients\" + 0.005*\"one\"'), (14, '0.006*\"israel\" + 0.006*\"university\" + 0.005*\"pittsburgh\" + 0.005*\"article\" + 0.004*\"team\" + 0.004*\"one\" + 0.004*\"division\" + 0.004*\"writes\" + 0.004*\"chicago\" + 0.004*\"first\"'), (15, '0.012*\"writes\" + 0.011*\"article\" + 0.005*\"one\" + 0.005*\"dont\" + 0.005*\"university\" + 0.005*\"team\" + 0.004*\"hockey\" + 0.004*\"year\" + 0.003*\"like\" + 0.003*\"two\"'), (16, '0.008*\"one\" + 0.008*\"scsi\" + 0.007*\"people\" + 0.007*\"said\" + 0.006*\"dont\" + 0.005*\"like\" + 0.005*\"time\" + 0.004*\"went\" + 0.004*\"two\" + 0.004*\"didnt\"'), (17, '0.006*\"one\" + 0.004*\"new\" + 0.004*\"people\" + 0.004*\"first\" + 0.004*\"university\" + 0.003*\"dont\" + 0.003*\"said\" + 0.003*\"years\" + 0.003*\"turkish\" + 0.003*\"space\"'), (18, '0.015*\"god\" + 0.010*\"one\" + 0.007*\"people\" + 0.006*\"writes\" + 0.006*\"jesus\" + 0.006*\"dont\" + 0.005*\"believe\" + 0.005*\"article\" + 0.004*\"like\" + 0.004*\"bible\"'), (19, '0.009*\"article\" + 0.008*\"writes\" + 0.007*\"one\" + 0.006*\"dont\" + 0.006*\"year\" + 0.005*\"like\" + 0.004*\"first\" + 0.004*\"people\" + 0.004*\"better\" + 0.004*\"last\"')]\n",
      "\n",
      "\n",
      "\n",
      "Topic: 0 \n",
      "Words: ['university', 'game', 'one', 'like', 'article', 'writes', 'win', 'card', 'time', 'dont']\n",
      "Topic: 1 \n",
      "Words: ['pts', 'team', 'players', 'player', 'one', 'article', 'time', 'writes', 'like', 'aids']\n",
      "Topic: 2 \n",
      "Words: ['information', 'space', 'encryption', 'technology', 'data', 'one', 'university', 'new', 'mail', 'writes']\n",
      "Topic: 3 \n",
      "Words: ['sale', 'new', 'one', 'article', 'like', 'writes', 'window', 'price', 'university', 'way']\n",
      "Topic: 4 \n",
      "Words: ['file', 'entry', 'program', 'section', 'entries', 'information', 'send', 'files', 'info', 'faq']\n",
      "Topic: 5 \n",
      "Words: ['people', 'writes', 'article', 'one', 'dont', 'like', 'government', 'gun', 'well', 'state']\n",
      "Topic: 6 \n",
      "Words: ['software', 'program', 'image', 'available', 'server', 'graphics', 'file', 'code', 'color', 'version']\n",
      "Topic: 7 \n",
      "Words: ['bike', 'writes', 'dod', 'article', 'dont', 'dog', 'oil', 'road', 'one', 'well']\n",
      "Topic: 8 \n",
      "Words: ['key', 'one', 'encryption', 'writes', 'ripem', 'keys', 'article', 'system', 'security', 'public']\n",
      "Topic: 9 \n",
      "Words: ['article', 'like', 'one', 'writes', 'moon', 'high', 'space', 'earth', 'mars', 'audio']\n",
      "Topic: 10 \n",
      "Words: ['max_max', 'max', 'rlk', 'air', 'zei', 'bhj', 'qax_max', 'fij', 'wwg', 'giz']\n",
      "Topic: 11 \n",
      "Words: ['x_s', 'sc_', 'scx', 'captain', 'xterm', 'chz', 'deskjet', 'color', 'cx_s', 'mcx']\n",
      "Topic: 12 \n",
      "Words: ['drive', 'windows', 'system', 'problem', 'one', 'like', 'writes', 'article', 'dont', 'anyone']\n",
      "Topic: 13 \n",
      "Words: ['jpeg', 'insurance', 'keyboard', 'helmet', 'finland', 'canada', 'medical', 'health', 'patients', 'one']\n",
      "Topic: 14 \n",
      "Words: ['israel', 'university', 'pittsburgh', 'article', 'team', 'one', 'division', 'writes', 'chicago', 'first']\n",
      "Topic: 15 \n",
      "Words: ['writes', 'article', 'one', 'dont', 'university', 'team', 'hockey', 'year', 'like', 'two']\n",
      "Topic: 16 \n",
      "Words: ['one', 'scsi', 'people', 'said', 'dont', 'like', 'time', 'went', 'two', 'didnt']\n",
      "Topic: 17 \n",
      "Words: ['one', 'new', 'people', 'first', 'university', 'dont', 'said', 'years', 'turkish', 'space']\n",
      "Topic: 18 \n",
      "Words: ['god', 'one', 'people', 'writes', 'jesus', 'dont', 'believe', 'article', 'like', 'bible']\n",
      "Topic: 19 \n",
      "Words: ['article', 'writes', 'one', 'dont', 'year', 'like', 'first', 'people', 'better', 'last']\n"
     ]
    }
   ],
   "source": [
    "## part (B)\n",
    "# Convert to list\n",
    "data = df.content.values.tolist()\n",
    "content = []\n",
    "# Preprocess\n",
    "for row in data:\n",
    "        # remove some patterns\n",
    "        row = re.sub(r'(From:\\s+[^\\n]+\\n)', ' ', row)\n",
    "        row = re.sub(r'(([\\sA-Za-z0-9\\-]+)?[A|a]rchive-name:)', ' ', row)\n",
    "        row = re.sub(r'(Last-modified:[^\\n]+\\n)', ' ', row)\n",
    "        row = re.sub(r'(Version:[^\\n]+\\n)', ' ', row)\n",
    "        row = re.sub(r'(Reply-To:[^\\n]+\\n)', ' ', row)\n",
    "        row = re.sub(r'(Lines:[^\\n]+\\n)', ' ', row)\n",
    "        row = re.sub(r'(N[n|N][t|T][p|P]-Posting-Host:[^\\n]+\\n)', ' ', row)\n",
    "       \n",
    "        # remove emails\n",
    "        row = re.sub('\\S*@\\S*\\s?', '', row)  \n",
    "        # remove whitespaces  \n",
    "        row = re.sub('\\s+', ' ', row)  \n",
    "        # remove single quotes and other special characters\n",
    "        row = re.sub(\"['~’—“”»«›!©…•–]\", \"\", row)  \n",
    "        # remove digits\n",
    "        row = re.sub(\"\\d+\", \"\", row)\n",
    "        ## part (C)\n",
    "        # lowercase content, remove Punctuations and remove words which length are less than 3\n",
    "        row = simple_preprocess(str(row), deacc=True, min_len=3) \n",
    "        content.append(row)\n",
    "\n",
    "## part (D)\n",
    "# Build the bigram model\n",
    "phrase_model = Phrases(content, min_count=15, threshold=100) \n",
    "# use less RAM, faster processing\n",
    "#frozen_model = phrase_model.freeze()\n",
    "frozen_model = FrozenPhrases(phrase_model)\n",
    "\n",
    "## part (E)\n",
    "# extend stopwords of this dataset\n",
    "cachedStopWords  = stopwords.words('english')\n",
    "cachedStopWords.extend(['from', 'subject', 'organization', 'edu', 'use', 'not', 'would', 'say',\n",
    "                        'could', 'know', 'good', 'get', 'done', 'try', 'many', 'some', 'nice', 'thank',\n",
    "                        'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem',\n",
    "                        'run', 'need', 'even', 'right', 'even', 'also', 'may', 'take', 'come'])\n",
    "new_content = []\n",
    "# remove stopwords\n",
    "for row in content:\n",
    "  removed_stopwords = [word for word in row if  word  not in cachedStopWords]\n",
    "  new_content.append(removed_stopwords)  \n",
    "# Build bigrams\n",
    "bigrams = frozen_model[new_content]\n",
    "\n",
    "## part (F)\n",
    "# Create Dictionary\n",
    "dct  = Dictionary(bigrams)\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [dct.doc2bow(text) for text in bigrams]\n",
    "\n",
    "## part (G)\n",
    "# Train the model on the corpus\n",
    "lda = LdaModel(corpus, id2word=dct, num_topics=20)\n",
    "print(lda.print_topics(num_topics=20, num_words=10))\n",
    "print(\"\\n\\n\")\n",
    "for idx, topic in lda.print_topics(num_words=10 , num_topics=20):\n",
    "\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, [w.split('\"')[1] for w in topic.split(\"+\")]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
